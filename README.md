# Stop-word-Analysis


This project focuses on extracting and analyzing text data from a list of article URLs provided in an Excel file (`Input.xlsx`). The main objective is to automate the process of web scraping to collect article content and then perform textual analysis to compute various linguistic and sentiment-based metrics. The program first reads the input file containing `URL_ID` and `URL` columns, visits each URL, and extracts only the article title and body text. Non-relevant sections of the webpage such as headers, footers, advertisements, and navigation menus are ignored to ensure that the extracted data contains only meaningful content for analysis. Each article is saved as a separate text file in a folder named `extracted_articles`, using the `URL_ID` as the filename.

Once the data extraction phase is complete, the program processes each extracted text to calculate multiple predefined variables. These include **Positive Score**, **Negative Score**, **Polarity Score**, **Subjectivity Score**, **Average Sentence Length**, **Percentage of Complex Words**, **Fog Index**, **Average Number of Words per Sentence**, **Complex Word Count**, **Word Count**, **Syllable per Word**, **Personal Pronouns** count, and **Average Word Length**. The sentiment scores are calculated using reference lists of positive and negative words, while readability and complexity metrics are derived using natural language processing techniques and syllable counting methods. This ensures that both emotional tone and structural complexity of the text are measured accurately.

The output is generated in the exact format specified in the `Output Data Structure.xlsx` file. All original columns from `Input.xlsx` are preserved in the output along with the computed variables, and the results are saved in an Excel file (`output.xlsx`) for easy reference. The program uses Python libraries such as **BeautifulSoup** and **requests** for web scraping, **pandas** for data handling, and **nltk** along with **textstat** for text processing and analysis. Additional custom functions are implemented for tasks like cleaning the text, removing stopwords, detecting complex words, and identifying personal pronouns.

To run the project, place the `Input.xlsx` file in the root directory, ensure all dependencies are installed, and execute the `main.py` script. The script will automatically extract, process, and save the data without requiring any manual intervention during execution. A `requirements.txt` file is provided to make dependency installation straightforward. It is recommended to run the program in a virtual environment to avoid version conflicts. This project is intended for educational purposes and demonstrates the combined use of web scraping, text preprocessing, and statistical analysis in Python to process real-world unstructured data.
