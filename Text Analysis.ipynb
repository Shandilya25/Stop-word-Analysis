{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b86131a-3acb-49b4-a4d5-b4a5fa673404",
   "metadata": {},
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c732625-8696-4238-88a7-6bfab0812e46",
   "metadata": {},
   "source": [
    "### We can use beautiful soop for extracting the required data from a given URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53be9613-409e-426a-af90-1948b4aeabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article(url, url_id, folder_name=\"articles\"):\n",
    "    try:\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch URL: {url}, Status Code: {response.status_code}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"Untitled\"\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        article_text = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "        content = f\"{title}\\n\\n{article_text}\"\n",
    "        file_name = os.path.join(folder_name, f\"{url_id}.txt\")\n",
    "        with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "        print(f\"Article saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7b5ff9b-fc09-468c-a11f-137ee58ee703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Netclan20241159</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Netclan20241160</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Netclan20241161</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Netclan20241162</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Netclan20241163</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              URL_ID                                                URL\n",
       "0    Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...\n",
       "1    Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...\n",
       "2    Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...\n",
       "3    Netclan20241020  https://insights.blackcoffer.com/efficient-pro...\n",
       "4    Netclan20241021  https://insights.blackcoffer.com/development-o...\n",
       "..               ...                                                ...\n",
       "142  Netclan20241159  https://insights.blackcoffer.com/population-an...\n",
       "143  Netclan20241160  https://insights.blackcoffer.com/google-lsa-ap...\n",
       "144  Netclan20241161  https://insights.blackcoffer.com/healthcare-da...\n",
       "145  Netclan20241162  https://insights.blackcoffer.com/budget-sales-...\n",
       "146  Netclan20241163  https://insights.blackcoffer.com/amazon-buy-bo...\n",
       "\n",
       "[147 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"Input.xlsx - Sheet1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8db0dc9c-6ae4-435b-a94e-c77d96681656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article saved to articles\\Netclan20241017.txt\n",
      "1\n",
      "Article saved to articles\\Netclan20241018.txt\n",
      "2\n",
      "Article saved to articles\\Netclan20241019.txt\n",
      "3\n",
      "Article saved to articles\\Netclan20241020.txt\n",
      "4\n",
      "Article saved to articles\\Netclan20241021.txt\n",
      "5\n",
      "Article saved to articles\\Netclan20241022.txt\n",
      "6\n",
      "Article saved to articles\\Netclan20241023.txt\n",
      "7\n",
      "Article saved to articles\\Netclan20241024.txt\n",
      "8\n",
      "Article saved to articles\\Netclan20241025.txt\n",
      "9\n",
      "Article saved to articles\\Netclan20241026.txt\n",
      "10\n",
      "Article saved to articles\\Netclan20241027.txt\n",
      "11\n",
      "Article saved to articles\\Netclan20241028.txt\n",
      "12\n",
      "Article saved to articles\\Netclan20241029.txt\n",
      "13\n",
      "Article saved to articles\\Netclan20241030.txt\n",
      "14\n",
      "Article saved to articles\\Netclan20241031.txt\n",
      "15\n",
      "Article saved to articles\\Netclan20241032.txt\n",
      "16\n",
      "Article saved to articles\\Netclan20241033.txt\n",
      "17\n",
      "Article saved to articles\\Netclan20241034.txt\n",
      "18\n",
      "Article saved to articles\\Netclan20241035.txt\n",
      "19\n",
      "Article saved to articles\\Netclan20241036.txt\n",
      "20\n",
      "Article saved to articles\\Netclan20241037.txt\n",
      "21\n",
      "Article saved to articles\\Netclan20241038.txt\n",
      "22\n",
      "Article saved to articles\\Netclan20241039.txt\n",
      "23\n",
      "Article saved to articles\\Netclan20241040.txt\n",
      "24\n",
      "Article saved to articles\\Netclan20241041.txt\n",
      "25\n",
      "Article saved to articles\\Netclan20241042.txt\n",
      "26\n",
      "Article saved to articles\\Netclan20241043.txt\n",
      "27\n",
      "Article saved to articles\\Netclan20241044.txt\n",
      "28\n",
      "Article saved to articles\\Netclan20241045.txt\n",
      "29\n",
      "Article saved to articles\\Netclan20241046.txt\n",
      "30\n",
      "Article saved to articles\\Netclan20241047.txt\n",
      "31\n",
      "Article saved to articles\\Netclan20241048.txt\n",
      "32\n",
      "Article saved to articles\\Netclan20241049.txt\n",
      "33\n",
      "Article saved to articles\\Netclan20241050.txt\n",
      "34\n",
      "Article saved to articles\\Netclan20241051.txt\n",
      "35\n",
      "Article saved to articles\\Netclan20241052.txt\n",
      "36\n",
      "Article saved to articles\\Netclan20241053.txt\n",
      "37\n",
      "Article saved to articles\\Netclan20241054.txt\n",
      "38\n",
      "Article saved to articles\\Netclan20241055.txt\n",
      "39\n",
      "Article saved to articles\\Netclan20241056.txt\n",
      "40\n",
      "Article saved to articles\\Netclan20241057.txt\n",
      "41\n",
      "Article saved to articles\\Netclan20241058.txt\n",
      "42\n",
      "Article saved to articles\\Netclan20241059.txt\n",
      "43\n",
      "Article saved to articles\\Netclan20241060.txt\n",
      "44\n",
      "Article saved to articles\\Netclan20241061.txt\n",
      "45\n",
      "Article saved to articles\\Netclan20241062.txt\n",
      "46\n",
      "Article saved to articles\\Netclan20241063.txt\n",
      "47\n",
      "Article saved to articles\\Netclan20241064.txt\n",
      "48\n",
      "Article saved to articles\\Netclan20241065.txt\n",
      "49\n",
      "Article saved to articles\\Netclan20241066.txt\n",
      "50\n",
      "Article saved to articles\\Netclan20241067.txt\n",
      "51\n",
      "Article saved to articles\\Netclan20241068.txt\n",
      "52\n",
      "Article saved to articles\\Netclan20241069.txt\n",
      "53\n",
      "Article saved to articles\\Netclan20241070.txt\n",
      "54\n",
      "Article saved to articles\\Netclan20241071.txt\n",
      "55\n",
      "Article saved to articles\\Netclan20241072.txt\n",
      "56\n",
      "Article saved to articles\\Netclan20241073.txt\n",
      "57\n",
      "Article saved to articles\\Netclan20241074.txt\n",
      "58\n",
      "Article saved to articles\\Netclan20241075.txt\n",
      "59\n",
      "Article saved to articles\\Netclan20241076.txt\n",
      "60\n",
      "Article saved to articles\\Netclan20241077.txt\n",
      "61\n",
      "Article saved to articles\\Netclan20241078.txt\n",
      "62\n",
      "Article saved to articles\\Netclan20241079.txt\n",
      "63\n",
      "Article saved to articles\\Netclan20241080.txt\n",
      "64\n",
      "Article saved to articles\\Netclan20241081.txt\n",
      "65\n",
      "Article saved to articles\\Netclan20241082.txt\n",
      "66\n",
      "Article saved to articles\\Netclan20241083.txt\n",
      "67\n",
      "Article saved to articles\\Netclan20241084.txt\n",
      "68\n",
      "Article saved to articles\\Netclan20241085.txt\n",
      "69\n",
      "Article saved to articles\\Netclan20241086.txt\n",
      "70\n",
      "Article saved to articles\\Netclan20241087.txt\n",
      "71\n",
      "Article saved to articles\\Netclan20241088.txt\n",
      "72\n",
      "Article saved to articles\\Netclan20241089.txt\n",
      "73\n",
      "Article saved to articles\\Netclan20241090.txt\n",
      "74\n",
      "Article saved to articles\\Netclan20241091.txt\n",
      "75\n",
      "Article saved to articles\\Netclan20241092.txt\n",
      "76\n",
      "Article saved to articles\\Netclan20241093.txt\n",
      "77\n",
      "Article saved to articles\\Netclan20241094.txt\n",
      "78\n",
      "Article saved to articles\\Netclan20241095.txt\n",
      "79\n",
      "Article saved to articles\\Netclan20241096.txt\n",
      "80\n",
      "Article saved to articles\\Netclan20241097.txt\n",
      "81\n",
      "Article saved to articles\\Netclan20241098.txt\n",
      "82\n",
      "Article saved to articles\\Netclan20241099.txt\n",
      "83\n",
      "Article saved to articles\\Netclan20241100.txt\n",
      "84\n",
      "Article saved to articles\\Netclan20241101.txt\n",
      "85\n",
      "Article saved to articles\\Netclan20241102.txt\n",
      "86\n",
      "Article saved to articles\\Netclan20241103.txt\n",
      "87\n",
      "Article saved to articles\\Netclan20241104.txt\n",
      "88\n",
      "Article saved to articles\\Netclan20241105.txt\n",
      "89\n",
      "Article saved to articles\\Netclan20241106.txt\n",
      "90\n",
      "Article saved to articles\\Netclan20241107.txt\n",
      "91\n",
      "Article saved to articles\\Netclan20241108.txt\n",
      "92\n",
      "Article saved to articles\\Netclan20241109.txt\n",
      "93\n",
      "Article saved to articles\\Netclan20241110.txt\n",
      "94\n",
      "Article saved to articles\\Netclan20241111.txt\n",
      "95\n",
      "Article saved to articles\\Netclan20241112.txt\n",
      "96\n",
      "Article saved to articles\\Netclan20241113.txt\n",
      "97\n",
      "Article saved to articles\\Netclan20241114.txt\n",
      "98\n",
      "Article saved to articles\\Netclan20241115.txt\n",
      "99\n",
      "Article saved to articles\\Netclan20241116.txt\n",
      "100\n",
      "Article saved to articles\\Netclan20241117.txt\n",
      "101\n",
      "Article saved to articles\\Netclan20241118.txt\n",
      "102\n",
      "Article saved to articles\\Netclan20241119.txt\n",
      "103\n",
      "Article saved to articles\\Netclan20241120.txt\n",
      "104\n",
      "Article saved to articles\\Netclan20241121.txt\n",
      "105\n",
      "Article saved to articles\\Netclan20241122.txt\n",
      "106\n",
      "Article saved to articles\\Netclan20241123.txt\n",
      "107\n",
      "Article saved to articles\\Netclan20241124.txt\n",
      "108\n",
      "Article saved to articles\\Netclan20241125.txt\n",
      "109\n",
      "Article saved to articles\\Netclan20241126.txt\n",
      "110\n",
      "Article saved to articles\\Netclan20241127.txt\n",
      "111\n",
      "Article saved to articles\\Netclan20241128.txt\n",
      "112\n",
      "Article saved to articles\\Netclan20241129.txt\n",
      "113\n",
      "Article saved to articles\\Netclan20241130.txt\n",
      "114\n",
      "Article saved to articles\\Netclan20241131.txt\n",
      "115\n",
      "Article saved to articles\\Netclan20241132.txt\n",
      "116\n",
      "Article saved to articles\\Netclan20241133.txt\n",
      "117\n",
      "Article saved to articles\\Netclan20241134.txt\n",
      "118\n",
      "Article saved to articles\\Netclan20241135.txt\n",
      "119\n",
      "Article saved to articles\\Netclan20241136.txt\n",
      "120\n",
      "Article saved to articles\\Netclan20241137.txt\n",
      "121\n",
      "Article saved to articles\\Netclan20241138.txt\n",
      "122\n",
      "Article saved to articles\\Netclan20241139.txt\n",
      "123\n",
      "Article saved to articles\\Netclan20241140.txt\n",
      "124\n",
      "Article saved to articles\\Netclan20241141.txt\n",
      "125\n",
      "Article saved to articles\\Netclan20241142.txt\n",
      "126\n",
      "Article saved to articles\\Netclan20241143.txt\n",
      "127\n",
      "Article saved to articles\\Netclan20241144.txt\n",
      "128\n",
      "Article saved to articles\\Netclan20241145.txt\n",
      "129\n",
      "Article saved to articles\\Netclan20241146.txt\n",
      "130\n",
      "Article saved to articles\\Netclan20241147.txt\n",
      "131\n",
      "Article saved to articles\\Netclan20241148.txt\n",
      "132\n",
      "Article saved to articles\\Netclan20241149.txt\n",
      "133\n",
      "Article saved to articles\\Netclan20241150.txt\n",
      "134\n",
      "Article saved to articles\\Netclan20241151.txt\n",
      "135\n",
      "Article saved to articles\\Netclan20241152.txt\n",
      "136\n",
      "Article saved to articles\\Netclan20241153.txt\n",
      "137\n",
      "Article saved to articles\\Netclan20241154.txt\n",
      "138\n",
      "Article saved to articles\\Netclan20241155.txt\n",
      "139\n",
      "Article saved to articles\\Netclan20241156.txt\n",
      "140\n",
      "Article saved to articles\\Netclan20241157.txt\n",
      "141\n",
      "Article saved to articles\\Netclan20241158.txt\n",
      "142\n",
      "Article saved to articles\\Netclan20241159.txt\n",
      "143\n",
      "Article saved to articles\\Netclan20241160.txt\n",
      "144\n",
      "Article saved to articles\\Netclan20241161.txt\n",
      "145\n",
      "Article saved to articles\\Netclan20241162.txt\n",
      "146\n",
      "Article saved to articles\\Netclan20241163.txt\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']  \n",
    "    url = row['URL']       \n",
    "    \n",
    "    extract_article(url, url_id) \n",
    "    print(index+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd0feb-5811-4333-9f5b-23f1320af9c6",
   "metadata": {},
   "source": [
    "## Token Generation and Filtering the Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4536b9e-8df4-4c4a-8394-188c096972ca",
   "metadata": {},
   "source": [
    "### For diving the text data into tokens we can use nltk package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc4cba0-7337-4656-afdc-68de3d5961ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d5d2b3-61de-46ab-b815-b697417207b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd0d7bc-16e0-48e5-8a26-76866b4316f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ed586-346a-4cc7-92ea-d822ff0df446",
   "metadata": {},
   "source": [
    "### Tokenizing and removing ,filtering using stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99833d-1ae5-4b46-a9c9-1672c71dac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: StopWords_Auditor.txt\n",
      "Loaded 8 words from StopWords_Auditor.txt\n",
      "Reading file: StopWords_Currencies.txt\n",
      "Loaded 85 words from StopWords_Currencies.txt\n",
      "Reading file: StopWords_DatesandNumbers.txt\n",
      "Loaded 109 words from StopWords_DatesandNumbers.txt\n",
      "Reading file: StopWords_Generic.txt\n",
      "Loaded 121 words from StopWords_Generic.txt\n",
      "Reading file: StopWords_GenericLong.txt\n",
      "Loaded 571 words from StopWords_GenericLong.txt\n",
      "Reading file: StopWords_Geographic.txt\n",
      "Loaded 199 words from StopWords_Geographic.txt\n",
      "Reading file: StopWords_Names.txt\n",
      "Loaded 13014 words from StopWords_Names.txt\n",
      "Total stop words loaded: 12768\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_stop_words(folder_path):\n",
    "    stop_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path) and file_name.endswith('.txt'):  # Ensure only .txt files are read\n",
    "            print(f\"Reading file: {file_name}\")  # Debug: Print the file being processed\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    words = file.read().splitlines()\n",
    "                    valid_words = [word.strip().lower() for word in words if word.strip()]\n",
    "                    print(f\"Loaded {len(valid_words)} words from {file_name}\")  # Debug: Number of words loaded\n",
    "                    stop_words.update(valid_words)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_name}: {e}\")\n",
    "    print(f\"Total stop words loaded: {len(stop_words)}\")  # Debug: Total stop words loaded\n",
    "    return stop_words\n",
    "\n",
    "def filter_stop_words(text, stop_words):\n",
    "    tokens = text.split() \n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Example Usage\n",
    "stop_words_folder = \"StopWords\"  # Replace with the correct folder path\n",
    "stop_words = load_stop_words(stop_words_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a4bd17-cd08-43f5-9725-e96dafc836eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48342160-15bd-4cc0-ba4a-54bf9571562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: StopWords_Auditor.txt\n",
      "Loaded 8 words from StopWords_Auditor.txt\n",
      "Reading file: StopWords_Currencies.txt\n",
      "Loaded 85 words from StopWords_Currencies.txt\n",
      "Reading file: StopWords_DatesandNumbers.txt\n",
      "Loaded 109 words from StopWords_DatesandNumbers.txt\n",
      "Reading file: StopWords_Generic.txt\n",
      "Loaded 121 words from StopWords_Generic.txt\n",
      "Reading file: StopWords_GenericLong.txt\n",
      "Loaded 571 words from StopWords_GenericLong.txt\n",
      "Reading file: StopWords_Geographic.txt\n",
      "Loaded 199 words from StopWords_Geographic.txt\n",
      "Reading file: StopWords_Names.txt\n",
      "Loaded 13014 words from StopWords_Names.txt\n",
      "Total stop words loaded: 12768\n",
      "Processed and saved: Netclan20241017.txt\n",
      "Processed and saved: Netclan20241018.txt\n",
      "Processed and saved: Netclan20241019.txt\n",
      "Processed and saved: Netclan20241020.txt\n",
      "Processed and saved: Netclan20241021.txt\n",
      "Processed and saved: Netclan20241022.txt\n",
      "Processed and saved: Netclan20241023.txt\n",
      "Processed and saved: Netclan20241024.txt\n",
      "Processed and saved: Netclan20241025.txt\n",
      "Processed and saved: Netclan20241026.txt\n",
      "Processed and saved: Netclan20241027.txt\n",
      "Processed and saved: Netclan20241028.txt\n",
      "Processed and saved: Netclan20241029.txt\n",
      "Processed and saved: Netclan20241030.txt\n",
      "Processed and saved: Netclan20241031.txt\n",
      "Processed and saved: Netclan20241032.txt\n",
      "Processed and saved: Netclan20241033.txt\n",
      "Processed and saved: Netclan20241034.txt\n",
      "Processed and saved: Netclan20241035.txt\n",
      "Processed and saved: Netclan20241036.txt\n",
      "Processed and saved: Netclan20241037.txt\n",
      "Processed and saved: Netclan20241038.txt\n",
      "Processed and saved: Netclan20241039.txt\n",
      "Processed and saved: Netclan20241040.txt\n",
      "Processed and saved: Netclan20241041.txt\n",
      "Processed and saved: Netclan20241042.txt\n",
      "Processed and saved: Netclan20241043.txt\n",
      "Processed and saved: Netclan20241044.txt\n",
      "Processed and saved: Netclan20241045.txt\n",
      "Processed and saved: Netclan20241046.txt\n",
      "Processed and saved: Netclan20241047.txt\n",
      "Processed and saved: Netclan20241048.txt\n",
      "Processed and saved: Netclan20241049.txt\n",
      "Processed and saved: Netclan20241050.txt\n",
      "Processed and saved: Netclan20241051.txt\n",
      "Processed and saved: Netclan20241052.txt\n",
      "Processed and saved: Netclan20241053.txt\n",
      "Processed and saved: Netclan20241054.txt\n",
      "Processed and saved: Netclan20241055.txt\n",
      "Processed and saved: Netclan20241056.txt\n",
      "Processed and saved: Netclan20241057.txt\n",
      "Processed and saved: Netclan20241058.txt\n",
      "Processed and saved: Netclan20241059.txt\n",
      "Processed and saved: Netclan20241060.txt\n",
      "Processed and saved: Netclan20241061.txt\n",
      "Processed and saved: Netclan20241062.txt\n",
      "Processed and saved: Netclan20241063.txt\n",
      "Processed and saved: Netclan20241064.txt\n",
      "Processed and saved: Netclan20241065.txt\n",
      "Processed and saved: Netclan20241066.txt\n",
      "Processed and saved: Netclan20241067.txt\n",
      "Processed and saved: Netclan20241068.txt\n",
      "Processed and saved: Netclan20241069.txt\n",
      "Processed and saved: Netclan20241070.txt\n",
      "Processed and saved: Netclan20241071.txt\n",
      "Processed and saved: Netclan20241072.txt\n",
      "Processed and saved: Netclan20241073.txt\n",
      "Processed and saved: Netclan20241074.txt\n",
      "Processed and saved: Netclan20241075.txt\n",
      "Processed and saved: Netclan20241076.txt\n",
      "Processed and saved: Netclan20241077.txt\n",
      "Processed and saved: Netclan20241078.txt\n",
      "Processed and saved: Netclan20241079.txt\n",
      "Processed and saved: Netclan20241080.txt\n",
      "Processed and saved: Netclan20241081.txt\n",
      "Processed and saved: Netclan20241082.txt\n",
      "Processed and saved: Netclan20241083.txt\n",
      "Processed and saved: Netclan20241084.txt\n",
      "Processed and saved: Netclan20241085.txt\n",
      "Processed and saved: Netclan20241086.txt\n",
      "Processed and saved: Netclan20241087.txt\n",
      "Processed and saved: Netclan20241088.txt\n",
      "Processed and saved: Netclan20241089.txt\n",
      "Processed and saved: Netclan20241090.txt\n",
      "Processed and saved: Netclan20241091.txt\n",
      "Processed and saved: Netclan20241092.txt\n",
      "Processed and saved: Netclan20241093.txt\n",
      "Processed and saved: Netclan20241094.txt\n",
      "Processed and saved: Netclan20241095.txt\n",
      "Processed and saved: Netclan20241096.txt\n",
      "Processed and saved: Netclan20241097.txt\n",
      "Processed and saved: Netclan20241098.txt\n",
      "Processed and saved: Netclan20241099.txt\n",
      "Processed and saved: Netclan20241100.txt\n",
      "Processed and saved: Netclan20241101.txt\n",
      "Processed and saved: Netclan20241102.txt\n",
      "Processed and saved: Netclan20241103.txt\n",
      "Processed and saved: Netclan20241104.txt\n",
      "Processed and saved: Netclan20241105.txt\n",
      "Processed and saved: Netclan20241106.txt\n",
      "Processed and saved: Netclan20241107.txt\n",
      "Processed and saved: Netclan20241108.txt\n",
      "Processed and saved: Netclan20241109.txt\n",
      "Processed and saved: Netclan20241110.txt\n",
      "Processed and saved: Netclan20241111.txt\n",
      "Processed and saved: Netclan20241112.txt\n",
      "Processed and saved: Netclan20241113.txt\n",
      "Processed and saved: Netclan20241114.txt\n",
      "Processed and saved: Netclan20241115.txt\n",
      "Processed and saved: Netclan20241116.txt\n",
      "Processed and saved: Netclan20241117.txt\n",
      "Processed and saved: Netclan20241118.txt\n",
      "Processed and saved: Netclan20241119.txt\n",
      "Processed and saved: Netclan20241120.txt\n",
      "Processed and saved: Netclan20241121.txt\n",
      "Processed and saved: Netclan20241122.txt\n",
      "Processed and saved: Netclan20241123.txt\n",
      "Processed and saved: Netclan20241124.txt\n",
      "Processed and saved: Netclan20241125.txt\n",
      "Processed and saved: Netclan20241126.txt\n",
      "Processed and saved: Netclan20241127.txt\n",
      "Processed and saved: Netclan20241128.txt\n",
      "Processed and saved: Netclan20241129.txt\n",
      "Processed and saved: Netclan20241130.txt\n",
      "Processed and saved: Netclan20241131.txt\n",
      "Processed and saved: Netclan20241132.txt\n",
      "Processed and saved: Netclan20241133.txt\n",
      "Processed and saved: Netclan20241134.txt\n",
      "Processed and saved: Netclan20241135.txt\n",
      "Processed and saved: Netclan20241136.txt\n",
      "Processed and saved: Netclan20241137.txt\n",
      "Processed and saved: Netclan20241138.txt\n",
      "Processed and saved: Netclan20241139.txt\n",
      "Processed and saved: Netclan20241140.txt\n",
      "Processed and saved: Netclan20241141.txt\n",
      "Processed and saved: Netclan20241142.txt\n",
      "Processed and saved: Netclan20241143.txt\n",
      "Processed and saved: Netclan20241144.txt\n",
      "Processed and saved: Netclan20241145.txt\n",
      "Processed and saved: Netclan20241146.txt\n",
      "Processed and saved: Netclan20241147.txt\n",
      "Processed and saved: Netclan20241148.txt\n",
      "Processed and saved: Netclan20241149.txt\n",
      "Processed and saved: Netclan20241150.txt\n",
      "Processed and saved: Netclan20241151.txt\n",
      "Processed and saved: Netclan20241152.txt\n",
      "Processed and saved: Netclan20241153.txt\n",
      "Processed and saved: Netclan20241154.txt\n",
      "Processed and saved: Netclan20241155.txt\n",
      "Processed and saved: Netclan20241156.txt\n",
      "Processed and saved: Netclan20241157.txt\n",
      "Processed and saved: Netclan20241158.txt\n",
      "Processed and saved: Netclan20241159.txt\n",
      "Processed and saved: Netclan20241160.txt\n",
      "Processed and saved: Netclan20241161.txt\n",
      "Processed and saved: Netclan20241162.txt\n",
      "Processed and saved: Netclan20241163.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "def tokenize(document, stopwords):\n",
    "    document = document.lower()\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    tokens = [token for token in tokenizer.tokenize(document) if token not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_file(file_location, stopwords):\n",
    "    with open(file_location, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        document = file.read()\n",
    "    tokens = tokenize(document, stopwords)\n",
    "    return tokens\n",
    "\n",
    "def process_folder(input_folder, stopwords, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            tokens = tokenize_file(file_path, stopwords)\n",
    "            \n",
    "            output_file_path = os.path.join(output_folder, file_name)  # Save with the same name\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(' '.join(tokens))\n",
    "            \n",
    "            print(f\"Processed and saved: {file_name}\")\n",
    "\n",
    "stop_words_folder = \"StopWords\" \n",
    "input_folder = \"articles\"  \n",
    "output_folder = \"CleanedTextFiles\"  \n",
    "\n",
    "stopwords = load_stop_words(stop_words_folder)\n",
    "process_folder(input_folder, stopwords, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8415d021-45ef-4367-b9bb-dba07b00589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2006 positive words.\n",
      "Loaded 4783 negative words.\n",
      "Combined dictionary has 6786 words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_master_dictionary(positive_file, negative_file):\n",
    "    master_dictionary = set()  # Use a set for fast lookup\n",
    "    for file_path in [positive_file, negative_file]:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            words = file.read().splitlines()\n",
    "            master_dictionary.update(word.strip().lower() for word in words if word.strip())  # Remove empty lines\n",
    "    return master_dictionary\n",
    "\n",
    "def load_word_list(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        return [word.strip().lower() for word in file.read().splitlines() if word.strip()]  # Remove empty lines\n",
    "\n",
    "positive_file = \"MasterDictionary/MasterDictionary/positive-words.txt\"  \n",
    "negative_file = \"MasterDictionary/MasterDictionary/negative-words.txt\" \n",
    "master_dictionary = load_master_dictionary(positive_file, negative_file)\n",
    "positive_word_list = load_word_list(positive_file)\n",
    "negative_word_list = load_word_list(negative_file)\n",
    "print(f\"Loaded {len(positive_word_list)} positive words.\")\n",
    "print(f\"Loaded {len(negative_word_list)} negative words.\")\n",
    "print(f\"Combined dictionary has {len(master_dictionary)} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de92f2f6-8869-4cb4-93ed-0d09d0ca19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Netclan20241017.txt: 8 positive words, 1 negative words.\n",
      "Processed Netclan20241018.txt: 15 positive words, 8 negative words.\n",
      "Processed Netclan20241019.txt: 13 positive words, 3 negative words.\n",
      "Processed Netclan20241020.txt: 26 positive words, 12 negative words.\n",
      "Processed Netclan20241021.txt: 5 positive words, 1 negative words.\n",
      "Processed Netclan20241022.txt: 8 positive words, 1 negative words.\n",
      "Processed Netclan20241023.txt: 15 positive words, 8 negative words.\n",
      "Processed Netclan20241024.txt: 13 positive words, 3 negative words.\n",
      "Processed Netclan20241025.txt: 26 positive words, 12 negative words.\n",
      "Processed Netclan20241026.txt: 59 positive words, 22 negative words.\n",
      "Processed Netclan20241027.txt: 21 positive words, 7 negative words.\n",
      "Processed Netclan20241028.txt: 30 positive words, 3 negative words.\n",
      "Processed Netclan20241029.txt: 23 positive words, 6 negative words.\n",
      "Processed Netclan20241030.txt: 16 positive words, 6 negative words.\n",
      "Processed Netclan20241031.txt: 64 positive words, 17 negative words.\n",
      "Processed Netclan20241032.txt: 10 positive words, 3 negative words.\n",
      "Processed Netclan20241033.txt: 42 positive words, 8 negative words.\n",
      "Processed Netclan20241034.txt: 8 positive words, 4 negative words.\n",
      "Processed Netclan20241035.txt: 17 positive words, 5 negative words.\n",
      "Processed Netclan20241036.txt: 23 positive words, 8 negative words.\n",
      "Processed Netclan20241037.txt: 14 positive words, 2 negative words.\n",
      "Processed Netclan20241038.txt: 13 positive words, 2 negative words.\n",
      "Processed Netclan20241039.txt: 13 positive words, 2 negative words.\n",
      "Processed Netclan20241040.txt: 20 positive words, 1 negative words.\n",
      "Processed Netclan20241041.txt: 22 positive words, 2 negative words.\n",
      "Processed Netclan20241042.txt: 14 positive words, 9 negative words.\n",
      "Processed Netclan20241043.txt: 12 positive words, 1 negative words.\n",
      "Processed Netclan20241044.txt: 20 positive words, 1 negative words.\n",
      "Processed Netclan20241045.txt: 16 positive words, 5 negative words.\n",
      "Processed Netclan20241046.txt: 24 positive words, 5 negative words.\n",
      "Processed Netclan20241047.txt: 11 positive words, 3 negative words.\n",
      "Processed Netclan20241048.txt: 50 positive words, 15 negative words.\n",
      "Processed Netclan20241049.txt: 18 positive words, 3 negative words.\n",
      "Processed Netclan20241050.txt: 22 positive words, 6 negative words.\n",
      "Processed Netclan20241051.txt: 33 positive words, 26 negative words.\n",
      "Processed Netclan20241052.txt: 17 positive words, 20 negative words.\n",
      "Processed Netclan20241053.txt: 20 positive words, 7 negative words.\n",
      "Processed Netclan20241054.txt: 15 positive words, 2 negative words.\n",
      "Processed Netclan20241055.txt: 48 positive words, 4 negative words.\n",
      "Processed Netclan20241056.txt: 3 positive words, 1 negative words.\n",
      "Processed Netclan20241057.txt: 13 positive words, 2 negative words.\n",
      "Processed Netclan20241058.txt: 13 positive words, 1 negative words.\n",
      "Processed Netclan20241059.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241060.txt: 14 positive words, 4 negative words.\n",
      "Processed Netclan20241061.txt: 19 positive words, 7 negative words.\n",
      "Processed Netclan20241062.txt: 7 positive words, 4 negative words.\n",
      "Processed Netclan20241063.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241064.txt: 6 positive words, 1 negative words.\n",
      "Processed Netclan20241065.txt: 7 positive words, 3 negative words.\n",
      "Processed Netclan20241066.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241067.txt: 9 positive words, 2 negative words.\n",
      "Processed Netclan20241068.txt: 13 positive words, 1 negative words.\n",
      "Processed Netclan20241069.txt: 6 positive words, 2 negative words.\n",
      "Processed Netclan20241070.txt: 6 positive words, 2 negative words.\n",
      "Processed Netclan20241071.txt: 11 positive words, 6 negative words.\n",
      "Processed Netclan20241072.txt: 13 positive words, 7 negative words.\n",
      "Processed Netclan20241073.txt: 13 positive words, 4 negative words.\n",
      "Processed Netclan20241074.txt: 14 positive words, 4 negative words.\n",
      "Processed Netclan20241075.txt: 8 positive words, 4 negative words.\n",
      "Processed Netclan20241076.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241077.txt: 7 positive words, 4 negative words.\n",
      "Processed Netclan20241078.txt: 9 positive words, 4 negative words.\n",
      "Processed Netclan20241079.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241080.txt: 7 positive words, 8 negative words.\n",
      "Processed Netclan20241081.txt: 12 positive words, 1 negative words.\n",
      "Processed Netclan20241082.txt: 13 positive words, 4 negative words.\n",
      "Processed Netclan20241083.txt: 10 positive words, 4 negative words.\n",
      "Processed Netclan20241084.txt: 23 positive words, 13 negative words.\n",
      "Processed Netclan20241085.txt: 10 positive words, 2 negative words.\n",
      "Processed Netclan20241086.txt: 14 positive words, 1 negative words.\n",
      "Processed Netclan20241087.txt: 28 positive words, 5 negative words.\n",
      "Processed Netclan20241088.txt: 7 positive words, 3 negative words.\n",
      "Processed Netclan20241089.txt: 8 positive words, 1 negative words.\n",
      "Processed Netclan20241090.txt: 5 positive words, 2 negative words.\n",
      "Processed Netclan20241091.txt: 10 positive words, 3 negative words.\n",
      "Processed Netclan20241092.txt: 7 positive words, 3 negative words.\n",
      "Processed Netclan20241093.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241094.txt: 5 positive words, 8 negative words.\n",
      "Processed Netclan20241095.txt: 10 positive words, 1 negative words.\n",
      "Processed Netclan20241096.txt: 5 positive words, 1 negative words.\n",
      "Processed Netclan20241097.txt: 5 positive words, 1 negative words.\n",
      "Processed Netclan20241098.txt: 12 positive words, 2 negative words.\n",
      "Processed Netclan20241099.txt: 8 positive words, 6 negative words.\n",
      "Processed Netclan20241100.txt: 13 positive words, 3 negative words.\n",
      "Processed Netclan20241101.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241102.txt: 6 positive words, 1 negative words.\n",
      "Processed Netclan20241103.txt: 5 positive words, 2 negative words.\n",
      "Processed Netclan20241104.txt: 6 positive words, 2 negative words.\n",
      "Processed Netclan20241105.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241106.txt: 10 positive words, 2 negative words.\n",
      "Processed Netclan20241107.txt: 4 positive words, 6 negative words.\n",
      "Processed Netclan20241108.txt: 18 positive words, 3 negative words.\n",
      "Processed Netclan20241109.txt: 18 positive words, 2 negative words.\n",
      "Processed Netclan20241110.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241111.txt: 7 positive words, 3 negative words.\n",
      "Processed Netclan20241112.txt: 7 positive words, 4 negative words.\n",
      "Processed Netclan20241113.txt: 6 positive words, 5 negative words.\n",
      "Processed Netclan20241114.txt: 4 positive words, 2 negative words.\n",
      "Processed Netclan20241115.txt: 9 positive words, 2 negative words.\n",
      "Processed Netclan20241116.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241117.txt: 7 positive words, 4 negative words.\n",
      "Processed Netclan20241118.txt: 4 positive words, 2 negative words.\n",
      "Processed Netclan20241119.txt: 6 positive words, 3 negative words.\n",
      "Processed Netclan20241120.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241121.txt: 20 positive words, 3 negative words.\n",
      "Processed Netclan20241122.txt: 9 positive words, 1 negative words.\n",
      "Processed Netclan20241123.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241124.txt: 8 positive words, 1 negative words.\n",
      "Processed Netclan20241125.txt: 10 positive words, 5 negative words.\n",
      "Processed Netclan20241126.txt: 9 positive words, 4 negative words.\n",
      "Processed Netclan20241127.txt: 9 positive words, 1 negative words.\n",
      "Processed Netclan20241128.txt: 9 positive words, 2 negative words.\n",
      "Processed Netclan20241129.txt: 18 positive words, 1 negative words.\n",
      "Processed Netclan20241130.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241131.txt: 9 positive words, 3 negative words.\n",
      "Processed Netclan20241132.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241133.txt: 11 positive words, 1 negative words.\n",
      "Processed Netclan20241134.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241135.txt: 3 positive words, 1 negative words.\n",
      "Processed Netclan20241136.txt: 6 positive words, 1 negative words.\n",
      "Processed Netclan20241137.txt: 9 positive words, 1 negative words.\n",
      "Processed Netclan20241138.txt: 6 positive words, 3 negative words.\n",
      "Processed Netclan20241139.txt: 7 positive words, 4 negative words.\n",
      "Processed Netclan20241140.txt: 6 positive words, 2 negative words.\n",
      "Processed Netclan20241141.txt: 12 positive words, 1 negative words.\n",
      "Processed Netclan20241142.txt: 4 positive words, 1 negative words.\n",
      "Processed Netclan20241143.txt: 6 positive words, 9 negative words.\n",
      "Processed Netclan20241144.txt: 12 positive words, 4 negative words.\n",
      "Processed Netclan20241145.txt: 15 positive words, 3 negative words.\n",
      "Processed Netclan20241146.txt: 12 positive words, 3 negative words.\n",
      "Processed Netclan20241147.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241148.txt: 7 positive words, 2 negative words.\n",
      "Processed Netclan20241149.txt: 5 positive words, 1 negative words.\n",
      "Processed Netclan20241150.txt: 5 positive words, 3 negative words.\n",
      "Processed Netclan20241151.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241152.txt: 7 positive words, 1 negative words.\n",
      "Processed Netclan20241153.txt: 6 positive words, 1 negative words.\n",
      "Processed Netclan20241154.txt: 5 positive words, 1 negative words.\n",
      "Processed Netclan20241155.txt: 6 positive words, 1 negative words.\n",
      "Processed Netclan20241156.txt: 5 positive words, 2 negative words.\n",
      "Processed Netclan20241157.txt: 14 positive words, 10 negative words.\n",
      "Processed Netclan20241158.txt: 12 positive words, 6 negative words.\n",
      "Processed Netclan20241159.txt: 15 positive words, 10 negative words.\n",
      "Processed Netclan20241160.txt: 23 positive words, 18 negative words.\n",
      "Processed Netclan20241161.txt: 9 positive words, 7 negative words.\n",
      "Processed Netclan20241162.txt: 3 positive words, 1 negative words.\n",
      "Processed Netclan20241163.txt: 5 positive words, 1 negative words.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_cleaned_files_with_lists(cleaned_folder, positive_words, negative_words, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for file_name in os.listdir(cleaned_folder):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(cleaned_folder, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                cleaned_text = file.read().split()  \n",
    "            positive_list = [word for word in cleaned_text if word in positive_words]\n",
    "            negative_list = [word for word in cleaned_text if word in negative_words]\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            positive_output_file = os.path.join(output_folder, f\"{base_name}_positive.txt\")\n",
    "            negative_output_file = os.path.join(output_folder, f\"{base_name}_negative.txt\")\n",
    "            \n",
    "            with open(positive_output_file, 'w', encoding='utf-8') as pos_file:\n",
    "                pos_file.write('\\n'.join(positive_list))\n",
    "            \n",
    "            with open(negative_output_file, 'w', encoding='utf-8') as neg_file:\n",
    "                neg_file.write('\\n'.join(negative_list))\n",
    "            \n",
    "            print(f\"Processed {file_name}: {len(positive_list)} positive words, {len(negative_list)} negative words.\")\n",
    "\n",
    "\n",
    "cleaned_folder = \"CleanedTextFiles\" \n",
    "output_folder = \"Processed_files\"   \n",
    "\n",
    "process_cleaned_files_with_lists(cleaned_folder, positive_word_list, negative_word_list, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35698b6f-ea43-4b65-9309-7341dd0da335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Total Words After Cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.035433</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>15</td>\n",
       "      <td>-8</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>13</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>26</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.074803</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Netclan20241159</td>\n",
       "      <td>15</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Netclan20241160</td>\n",
       "      <td>23</td>\n",
       "      <td>-18</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Netclan20241161</td>\n",
       "      <td>9</td>\n",
       "      <td>-7</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Netclan20241162</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Netclan20241163</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.031088</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           File Name  Positive Score  Negative Score  Polarity Score  \\\n",
       "0    Netclan20241017               8              -1        0.777778   \n",
       "1    Netclan20241018              15              -8        0.304348   \n",
       "2    Netclan20241019              13              -3        0.625000   \n",
       "3    Netclan20241020              26             -12        0.368421   \n",
       "4    Netclan20241021               5              -1        0.666667   \n",
       "..               ...             ...             ...             ...   \n",
       "142  Netclan20241159              15             -10        0.200000   \n",
       "143  Netclan20241160              23             -18        0.121951   \n",
       "144  Netclan20241161               9              -7        0.125000   \n",
       "145  Netclan20241162               3              -1        0.500000   \n",
       "146  Netclan20241163               5              -1        0.666667   \n",
       "\n",
       "     Subjectivity Score  Total Words After Cleaning  \n",
       "0              0.035433                         254  \n",
       "1              0.044146                         521  \n",
       "2              0.054795                         292  \n",
       "3              0.074803                         508  \n",
       "4              0.023529                         255  \n",
       "..                  ...                         ...  \n",
       "142            0.038168                         655  \n",
       "143            0.045759                         896  \n",
       "144            0.066667                         240  \n",
       "145            0.027972                         143  \n",
       "146            0.031088                         193  \n",
       "\n",
       "[147 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_scores_from_file(file_path, positive_words, negative_words):\n",
    "    # Read the file content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().split()\n",
    "    \n",
    "    # Calculate scores\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = -sum(1 for word in words if word in negative_words)  # Keep negative score as negative\n",
    "    \n",
    "    return positive_score, negative_score\n",
    "\n",
    "def get_total_words_after_cleaning(cleaned_folder, base_name):\n",
    "    # Locate the corresponding cleaned file\n",
    "    cleaned_file_path = os.path.join(cleaned_folder, f\"{base_name}.txt\")\n",
    "    if not os.path.exists(cleaned_file_path):\n",
    "        return 0  # Return 0 if cleaned file is missing\n",
    "    \n",
    "    # Count total words in the cleaned file\n",
    "    with open(cleaned_file_path, 'r', encoding='utf-8') as file:\n",
    "        words = file.read().split()\n",
    "    return len(words)\n",
    "\n",
    "def process_combined_scores(input_folder, cleaned_folder, positive_words, negative_words):\n",
    "    # Dictionary to store combined results\n",
    "    scores_dict = {}\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.txt'):  # Process only text files\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            \n",
    "            # Determine the base name (remove \"_positive\" or \"_negative\")\n",
    "            if \"_positive\" in file_name:\n",
    "                base_name = file_name.replace(\"_positive.txt\", \"\")\n",
    "            elif \"_negative\" in file_name:\n",
    "                base_name = file_name.replace(\"_negative.txt\", \"\")\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Calculate positive and negative scores\n",
    "            positive_score, negative_score = calculate_scores_from_file(file_path, positive_words, negative_words)\n",
    "            \n",
    "            # Add or update the scores in the dictionary\n",
    "            if base_name not in scores_dict:\n",
    "                scores_dict[base_name] = {\n",
    "                    \"Positive Score\": 0, \n",
    "                    \"Negative Score\": 0\n",
    "                }\n",
    "            scores_dict[base_name][\"Positive Score\"] += positive_score\n",
    "            scores_dict[base_name][\"Negative Score\"] += negative_score\n",
    "\n",
    "    # Create the DataFrame\n",
    "    data = []\n",
    "    for base_name, scores in scores_dict.items():\n",
    "        positive_score = scores[\"Positive Score\"]\n",
    "        negative_score = scores[\"Negative Score\"]\n",
    "        \n",
    "        # Get total words from the cleaned files\n",
    "        total_words_after_cleaning = get_total_words_after_cleaning(cleaned_folder, base_name)\n",
    "        \n",
    "        # Calculate Polarity Score and Subjectivity Score\n",
    "        polarity_score = (positive_score - abs(negative_score)) / ((positive_score + abs(negative_score)) + 0.000001)\n",
    "        subjectivity_score = (positive_score + abs(negative_score)) / (total_words_after_cleaning + 0.000001)\n",
    "        \n",
    "        # Append the data\n",
    "        data.append({\n",
    "            \"File Name\": base_name,\n",
    "            \"Positive Score\": positive_score,\n",
    "            \"Negative Score\": negative_score,\n",
    "            \"Polarity Score\": polarity_score,\n",
    "            \"Subjectivity Score\": subjectivity_score,\n",
    "            \"Total Words After Cleaning\": total_words_after_cleaning\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "     # Replace with your negative words list\n",
    "input_folder = \"Processed_files\"                    # Folder containing files with positive/negative words\n",
    "cleaned_folder = \"Cleanedtextfiles\"                       # Folder containing cleaned text files\n",
    "\n",
    "# Generate the combined DataFrame\n",
    "df_combined_scores = process_combined_scores(input_folder, cleaned_folder, positive_word_list, negative_word_list)\n",
    "\n",
    "# Save to a CSV file for reference\n",
    "df_combined_scores.to_csv(\"combined_scores_summary.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_combined_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a7015fe-8903-4b5d-abfb-a06eff32651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.3-cp39-cp39-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.11-cp39-cp39-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.10-cp39-cp39-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.3-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.0-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.2.0,>=1.1.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.1.0-cp39-cp39-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shand\\anaconda3\\envs\\pytorch_cuda_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.0-cp39-cp39-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.3-cp39-cp39-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.3 MB 799.2 kB/s eta 0:00:15\n",
      "   -- ------------------------------------- 0.8/12.3 MB 882.6 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/12.3 MB 882.6 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/12.3 MB 949.8 kB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 1.6/12.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.8/12.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.1/12.3 MB 1.2 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.6/12.3 MB 1.3 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.1/12.3 MB 1.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.7/12.3 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.2/12.3 MB 1.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.7/12.3 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.5/12.3 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.3/12.3 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.3 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.1/12.3 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.2/12.3 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.5/12.3 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.10-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.11-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.0-cp39-cp39-win_amd64.whl (633 kB)\n",
      "   ---------------------------------------- 0.0/633.4 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/633.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 633.4/633.4 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.3-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.1.0-cp39-cp39-win_amd64.whl (6.4 MB)\n",
      "   ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/6.4 MB 4.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.4 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.4 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.2/6.4 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.4/6.4 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.0-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed blis-1.1.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 preshed-3.0.9 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.3 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.0 thinc-8.3.3 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953bdea6-4109-403c-b035-0036a5a50a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.8 MB 6.9 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.2/12.8 MB 13.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 12.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dcc2482-c465-441c-af6d-0507350c537d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Average Words Per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>Avg Syllables Per Word</th>\n",
       "      <th>Syllable Counts Per Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Netclan20241017.txt</th>\n",
       "      <td>32.727273</td>\n",
       "      <td>26.388889</td>\n",
       "      <td>23.646465</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>5.936111</td>\n",
       "      <td>1.941667</td>\n",
       "      <td>1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241018.txt</th>\n",
       "      <td>27.105263</td>\n",
       "      <td>17.184466</td>\n",
       "      <td>17.715892</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>177</td>\n",
       "      <td>8</td>\n",
       "      <td>5.053398</td>\n",
       "      <td>1.657282</td>\n",
       "      <td>3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241019.txt</th>\n",
       "      <td>31.076923</td>\n",
       "      <td>22.029703</td>\n",
       "      <td>21.242650</td>\n",
       "      <td>31.076923</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>5.784653</td>\n",
       "      <td>1.866337</td>\n",
       "      <td>1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241020.txt</th>\n",
       "      <td>23.766667</td>\n",
       "      <td>33.239832</td>\n",
       "      <td>22.802599</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>237</td>\n",
       "      <td>5</td>\n",
       "      <td>6.375877</td>\n",
       "      <td>2.107994</td>\n",
       "      <td>3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241021.txt</th>\n",
       "      <td>26.142857</td>\n",
       "      <td>24.863388</td>\n",
       "      <td>20.402498</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>5.765027</td>\n",
       "      <td>1.923497</td>\n",
       "      <td>4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241159.txt</th>\n",
       "      <td>22.790698</td>\n",
       "      <td>21.938776</td>\n",
       "      <td>17.891789</td>\n",
       "      <td>22.790698</td>\n",
       "      <td>215</td>\n",
       "      <td>4</td>\n",
       "      <td>5.497959</td>\n",
       "      <td>1.845918</td>\n",
       "      <td>4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241160.txt</th>\n",
       "      <td>22.530303</td>\n",
       "      <td>16.879623</td>\n",
       "      <td>15.763971</td>\n",
       "      <td>22.530303</td>\n",
       "      <td>251</td>\n",
       "      <td>13</td>\n",
       "      <td>5.255548</td>\n",
       "      <td>1.715535</td>\n",
       "      <td>1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241161.txt</th>\n",
       "      <td>24.176471</td>\n",
       "      <td>18.248175</td>\n",
       "      <td>16.969858</td>\n",
       "      <td>24.176471</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>5.284672</td>\n",
       "      <td>1.749392</td>\n",
       "      <td>2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241162.txt</th>\n",
       "      <td>41.800000</td>\n",
       "      <td>28.708134</td>\n",
       "      <td>28.203254</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>6.325359</td>\n",
       "      <td>2.110048</td>\n",
       "      <td>2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netclan20241163.txt</th>\n",
       "      <td>25.583333</td>\n",
       "      <td>24.104235</td>\n",
       "      <td>19.875027</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>5.625407</td>\n",
       "      <td>1.882736</td>\n",
       "      <td>3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Average Sentence Length  Percentage of Complex Words  \\\n",
       "Netclan20241017.txt                32.727273                    26.388889   \n",
       "Netclan20241018.txt                27.105263                    17.184466   \n",
       "Netclan20241019.txt                31.076923                    22.029703   \n",
       "Netclan20241020.txt                23.766667                    33.239832   \n",
       "Netclan20241021.txt                26.142857                    24.863388   \n",
       "...                                      ...                          ...   \n",
       "Netclan20241159.txt                22.790698                    21.938776   \n",
       "Netclan20241160.txt                22.530303                    16.879623   \n",
       "Netclan20241161.txt                24.176471                    18.248175   \n",
       "Netclan20241162.txt                41.800000                    28.708134   \n",
       "Netclan20241163.txt                25.583333                    24.104235   \n",
       "\n",
       "                     Fog Index  Average Words Per Sentence  \\\n",
       "Netclan20241017.txt  23.646465                   32.727273   \n",
       "Netclan20241018.txt  17.715892                   27.105263   \n",
       "Netclan20241019.txt  21.242650                   31.076923   \n",
       "Netclan20241020.txt  22.802599                   23.766667   \n",
       "Netclan20241021.txt  20.402498                   26.142857   \n",
       "...                        ...                         ...   \n",
       "Netclan20241159.txt  17.891789                   22.790698   \n",
       "Netclan20241160.txt  15.763971                   22.530303   \n",
       "Netclan20241161.txt  16.969858                   24.176471   \n",
       "Netclan20241162.txt  28.203254                   41.800000   \n",
       "Netclan20241163.txt  19.875027                   25.583333   \n",
       "\n",
       "                     Complex Word Count  Personal Pronouns  \\\n",
       "Netclan20241017.txt                  95                  2   \n",
       "Netclan20241018.txt                 177                  8   \n",
       "Netclan20241019.txt                  89                  2   \n",
       "Netclan20241020.txt                 237                  5   \n",
       "Netclan20241021.txt                  91                  2   \n",
       "...                                 ...                ...   \n",
       "Netclan20241159.txt                 215                  4   \n",
       "Netclan20241160.txt                 251                 13   \n",
       "Netclan20241161.txt                  75                 12   \n",
       "Netclan20241162.txt                  60                  1   \n",
       "Netclan20241163.txt                  74                  2   \n",
       "\n",
       "                     Average Word Length  Avg Syllables Per Word  \\\n",
       "Netclan20241017.txt             5.936111                1.941667   \n",
       "Netclan20241018.txt             5.053398                1.657282   \n",
       "Netclan20241019.txt             5.784653                1.866337   \n",
       "Netclan20241020.txt             6.375877                2.107994   \n",
       "Netclan20241021.txt             5.765027                1.923497   \n",
       "...                                  ...                     ...   \n",
       "Netclan20241159.txt             5.497959                1.845918   \n",
       "Netclan20241160.txt             5.255548                1.715535   \n",
       "Netclan20241161.txt             5.284672                1.749392   \n",
       "Netclan20241162.txt             6.325359                2.110048   \n",
       "Netclan20241163.txt             5.625407                1.882736   \n",
       "\n",
       "                                              Syllable Counts Per Word  \n",
       "Netclan20241017.txt  1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...  \n",
       "Netclan20241018.txt  3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...  \n",
       "Netclan20241019.txt  1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...  \n",
       "Netclan20241020.txt  3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...  \n",
       "Netclan20241021.txt  4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "...                                                                ...  \n",
       "Netclan20241159.txt  4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...  \n",
       "Netclan20241160.txt  1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "Netclan20241161.txt  2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...  \n",
       "Netclan20241162.txt  2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "Netclan20241163.txt  3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...  \n",
       "\n",
       "[147 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load SpaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"Count the number of syllables in a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    if word.endswith(\"e\"):\n",
    "        word = word[:-1]  # Silent 'e'\n",
    "    for index, char in enumerate(word):\n",
    "        if char in vowels and (index == 0 or word[index - 1] not in vowels):\n",
    "            count += 1\n",
    "    # Handle exceptions for \"es\" and \"ed\" endings\n",
    "    if word.endswith((\"es\", \"ed\")) and len(word) > 2:\n",
    "        count = max(1, count - 1)\n",
    "    return max(1, count)  # Ensure at least one syllable\n",
    "\n",
    "def analyze_text(text):\n",
    "    \"\"\"Perform readability analysis on the given text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "\n",
    "    if sentence_count == 0:  # Handle edge case\n",
    "        return None\n",
    "    \n",
    "    # Average Sentence Length\n",
    "    avg_sentence_length = word_count / sentence_count\n",
    "\n",
    "    # Syllable Count Per Word\n",
    "    syllable_counts = [count_syllables(word) for word in words]\n",
    "    avg_syllables_per_word = sum(syllable_counts) / len(syllable_counts)\n",
    "\n",
    "    # Complex Word Count\n",
    "    complex_words = [word for word, syllables in zip(words, syllable_counts) if syllables > 2]\n",
    "    complex_word_count = len(complex_words)\n",
    "    percentage_complex_words = complex_word_count / word_count\n",
    "\n",
    "    # Fog Index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words * 100)\n",
    "\n",
    "    # Average Words Per Sentence\n",
    "    avg_words_per_sentence = word_count / sentence_count\n",
    "\n",
    "    # Personal Pronouns\n",
    "    pronouns = [\"i\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    personal_pronouns = sum(1 for word in words if word.lower() in pronouns)\n",
    "\n",
    "    # Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count\n",
    "\n",
    "    return {\n",
    "        \"Average Sentence Length\": avg_sentence_length,\n",
    "        \"Percentage of Complex Words\": percentage_complex_words * 100,\n",
    "        \"Fog Index\": fog_index,\n",
    "        \"Average Words Per Sentence\": avg_words_per_sentence,\n",
    "        \"Complex Word Count\": complex_word_count,\n",
    "        \"Personal Pronouns\": personal_pronouns,\n",
    "        \"Average Word Length\": avg_word_length,\n",
    "        \"Avg Syllables Per Word\": avg_syllables_per_word,\n",
    "        \"Syllable Counts\": syllable_counts,  # Added syllable counts for each word\n",
    "    }\n",
    "\n",
    "def analyze_folder(folder_path):\n",
    "    \"\"\"Analyze all text files in the folder and create a DataFrame.\"\"\"\n",
    "    results = []\n",
    "    file_names = []\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path) and file_name.endswith(\".txt\"):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                analysis = analyze_text(text)\n",
    "                if analysis:\n",
    "                    # Add syllable counts per word as a separate column\n",
    "                    syllable_counts_str = \", \".join(map(str, analysis.pop(\"Syllable Counts\")))\n",
    "                    analysis[\"Syllable Counts Per Word\"] = syllable_counts_str\n",
    "                    results.append(analysis)\n",
    "                    file_names.append(file_name)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results, index=file_names)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Folder path containing text files\n",
    "folder_path = \"articles\"\n",
    "\n",
    "# Analyze folder and display results\n",
    "df = analyze_folder(folder_path)\n",
    "\n",
    "# Save to CSV if needed\n",
    "df.to_csv(\"readability_analysis.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea12ba68-8fb4-4872-9e58-fbaedac8cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df_combined_scores.reset_index(drop=True), df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "155e309e-7cfb-49b4-88c5-8bdf2764f326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Total Words After Cleaning</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Average Words Per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>Avg Syllables Per Word</th>\n",
       "      <th>Syllable Counts Per Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.035433</td>\n",
       "      <td>254</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>26.388889</td>\n",
       "      <td>23.646465</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>5.936111</td>\n",
       "      <td>1.941667</td>\n",
       "      <td>1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>15</td>\n",
       "      <td>-8</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>521</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>17.184466</td>\n",
       "      <td>17.715892</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>177</td>\n",
       "      <td>8</td>\n",
       "      <td>5.053398</td>\n",
       "      <td>1.657282</td>\n",
       "      <td>3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>13</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>292</td>\n",
       "      <td>31.076923</td>\n",
       "      <td>22.029703</td>\n",
       "      <td>21.242650</td>\n",
       "      <td>31.076923</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>5.784653</td>\n",
       "      <td>1.866337</td>\n",
       "      <td>1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>26</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.074803</td>\n",
       "      <td>508</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>33.239832</td>\n",
       "      <td>22.802599</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>237</td>\n",
       "      <td>5</td>\n",
       "      <td>6.375877</td>\n",
       "      <td>2.107994</td>\n",
       "      <td>3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>255</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>24.863388</td>\n",
       "      <td>20.402498</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>5.765027</td>\n",
       "      <td>1.923497</td>\n",
       "      <td>4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Netclan20241159</td>\n",
       "      <td>15</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>655</td>\n",
       "      <td>22.790698</td>\n",
       "      <td>21.938776</td>\n",
       "      <td>17.891789</td>\n",
       "      <td>22.790698</td>\n",
       "      <td>215</td>\n",
       "      <td>4</td>\n",
       "      <td>5.497959</td>\n",
       "      <td>1.845918</td>\n",
       "      <td>4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Netclan20241160</td>\n",
       "      <td>23</td>\n",
       "      <td>-18</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>896</td>\n",
       "      <td>22.530303</td>\n",
       "      <td>16.879623</td>\n",
       "      <td>15.763971</td>\n",
       "      <td>22.530303</td>\n",
       "      <td>251</td>\n",
       "      <td>13</td>\n",
       "      <td>5.255548</td>\n",
       "      <td>1.715535</td>\n",
       "      <td>1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Netclan20241161</td>\n",
       "      <td>9</td>\n",
       "      <td>-7</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>240</td>\n",
       "      <td>24.176471</td>\n",
       "      <td>18.248175</td>\n",
       "      <td>16.969858</td>\n",
       "      <td>24.176471</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>5.284672</td>\n",
       "      <td>1.749392</td>\n",
       "      <td>2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Netclan20241162</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>143</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>28.708134</td>\n",
       "      <td>28.203254</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>6.325359</td>\n",
       "      <td>2.110048</td>\n",
       "      <td>2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Netclan20241163</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.031088</td>\n",
       "      <td>193</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>24.104235</td>\n",
       "      <td>19.875027</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>5.625407</td>\n",
       "      <td>1.882736</td>\n",
       "      <td>3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           File Name  Positive Score  Negative Score  Polarity Score  \\\n",
       "0    Netclan20241017               8              -1        0.777778   \n",
       "1    Netclan20241018              15              -8        0.304348   \n",
       "2    Netclan20241019              13              -3        0.625000   \n",
       "3    Netclan20241020              26             -12        0.368421   \n",
       "4    Netclan20241021               5              -1        0.666667   \n",
       "..               ...             ...             ...             ...   \n",
       "142  Netclan20241159              15             -10        0.200000   \n",
       "143  Netclan20241160              23             -18        0.121951   \n",
       "144  Netclan20241161               9              -7        0.125000   \n",
       "145  Netclan20241162               3              -1        0.500000   \n",
       "146  Netclan20241163               5              -1        0.666667   \n",
       "\n",
       "     Subjectivity Score  Total Words After Cleaning  Average Sentence Length  \\\n",
       "0              0.035433                         254                32.727273   \n",
       "1              0.044146                         521                27.105263   \n",
       "2              0.054795                         292                31.076923   \n",
       "3              0.074803                         508                23.766667   \n",
       "4              0.023529                         255                26.142857   \n",
       "..                  ...                         ...                      ...   \n",
       "142            0.038168                         655                22.790698   \n",
       "143            0.045759                         896                22.530303   \n",
       "144            0.066667                         240                24.176471   \n",
       "145            0.027972                         143                41.800000   \n",
       "146            0.031088                         193                25.583333   \n",
       "\n",
       "     Percentage of Complex Words  Fog Index  Average Words Per Sentence  \\\n",
       "0                      26.388889  23.646465                   32.727273   \n",
       "1                      17.184466  17.715892                   27.105263   \n",
       "2                      22.029703  21.242650                   31.076923   \n",
       "3                      33.239832  22.802599                   23.766667   \n",
       "4                      24.863388  20.402498                   26.142857   \n",
       "..                           ...        ...                         ...   \n",
       "142                    21.938776  17.891789                   22.790698   \n",
       "143                    16.879623  15.763971                   22.530303   \n",
       "144                    18.248175  16.969858                   24.176471   \n",
       "145                    28.708134  28.203254                   41.800000   \n",
       "146                    24.104235  19.875027                   25.583333   \n",
       "\n",
       "     Complex Word Count  Personal Pronouns  Average Word Length  \\\n",
       "0                    95                  2             5.936111   \n",
       "1                   177                  8             5.053398   \n",
       "2                    89                  2             5.784653   \n",
       "3                   237                  5             6.375877   \n",
       "4                    91                  2             5.765027   \n",
       "..                  ...                ...                  ...   \n",
       "142                 215                  4             5.497959   \n",
       "143                 251                 13             5.255548   \n",
       "144                  75                 12             5.284672   \n",
       "145                  60                  1             6.325359   \n",
       "146                  74                  2             5.625407   \n",
       "\n",
       "     Avg Syllables Per Word                           Syllable Counts Per Word  \n",
       "0                  1.941667  1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...  \n",
       "1                  1.657282  3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...  \n",
       "2                  1.866337  1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...  \n",
       "3                  2.107994  3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...  \n",
       "4                  1.923497  4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "..                      ...                                                ...  \n",
       "142                1.845918  4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...  \n",
       "143                1.715535  1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "144                1.749392  2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...  \n",
       "145                2.110048  2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "146                1.882736  3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...  \n",
       "\n",
       "[147 rows x 15 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af5c9247-a205-460c-a0de-0dc2876a018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rename(columns={\"Total Words After Cleaning\": \"Word Count\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f48e0976-a98e-48e9-a5c8-7d1e082c6179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Average Words Per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>Avg Syllables Per Word</th>\n",
       "      <th>Syllable Counts Per Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.035433</td>\n",
       "      <td>254</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>26.388889</td>\n",
       "      <td>23.646465</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>5.936111</td>\n",
       "      <td>1.941667</td>\n",
       "      <td>1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>15</td>\n",
       "      <td>-8</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>521</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>17.184466</td>\n",
       "      <td>17.715892</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>177</td>\n",
       "      <td>8</td>\n",
       "      <td>5.053398</td>\n",
       "      <td>1.657282</td>\n",
       "      <td>3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>13</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>292</td>\n",
       "      <td>31.076923</td>\n",
       "      <td>22.029703</td>\n",
       "      <td>21.242650</td>\n",
       "      <td>31.076923</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>5.784653</td>\n",
       "      <td>1.866337</td>\n",
       "      <td>1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>26</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.074803</td>\n",
       "      <td>508</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>33.239832</td>\n",
       "      <td>22.802599</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>237</td>\n",
       "      <td>5</td>\n",
       "      <td>6.375877</td>\n",
       "      <td>2.107994</td>\n",
       "      <td>3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>255</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>24.863388</td>\n",
       "      <td>20.402498</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>5.765027</td>\n",
       "      <td>1.923497</td>\n",
       "      <td>4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Netclan20241159</td>\n",
       "      <td>15</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>655</td>\n",
       "      <td>22.790698</td>\n",
       "      <td>21.938776</td>\n",
       "      <td>17.891789</td>\n",
       "      <td>22.790698</td>\n",
       "      <td>215</td>\n",
       "      <td>4</td>\n",
       "      <td>5.497959</td>\n",
       "      <td>1.845918</td>\n",
       "      <td>4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Netclan20241160</td>\n",
       "      <td>23</td>\n",
       "      <td>-18</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>896</td>\n",
       "      <td>22.530303</td>\n",
       "      <td>16.879623</td>\n",
       "      <td>15.763971</td>\n",
       "      <td>22.530303</td>\n",
       "      <td>251</td>\n",
       "      <td>13</td>\n",
       "      <td>5.255548</td>\n",
       "      <td>1.715535</td>\n",
       "      <td>1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Netclan20241161</td>\n",
       "      <td>9</td>\n",
       "      <td>-7</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>240</td>\n",
       "      <td>24.176471</td>\n",
       "      <td>18.248175</td>\n",
       "      <td>16.969858</td>\n",
       "      <td>24.176471</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>5.284672</td>\n",
       "      <td>1.749392</td>\n",
       "      <td>2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Netclan20241162</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>143</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>28.708134</td>\n",
       "      <td>28.203254</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>6.325359</td>\n",
       "      <td>2.110048</td>\n",
       "      <td>2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Netclan20241163</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.031088</td>\n",
       "      <td>193</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>24.104235</td>\n",
       "      <td>19.875027</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>5.625407</td>\n",
       "      <td>1.882736</td>\n",
       "      <td>3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           File Name  Positive Score  Negative Score  Polarity Score  \\\n",
       "0    Netclan20241017               8              -1        0.777778   \n",
       "1    Netclan20241018              15              -8        0.304348   \n",
       "2    Netclan20241019              13              -3        0.625000   \n",
       "3    Netclan20241020              26             -12        0.368421   \n",
       "4    Netclan20241021               5              -1        0.666667   \n",
       "..               ...             ...             ...             ...   \n",
       "142  Netclan20241159              15             -10        0.200000   \n",
       "143  Netclan20241160              23             -18        0.121951   \n",
       "144  Netclan20241161               9              -7        0.125000   \n",
       "145  Netclan20241162               3              -1        0.500000   \n",
       "146  Netclan20241163               5              -1        0.666667   \n",
       "\n",
       "     Subjectivity Score  Word Count  Average Sentence Length  \\\n",
       "0              0.035433         254                32.727273   \n",
       "1              0.044146         521                27.105263   \n",
       "2              0.054795         292                31.076923   \n",
       "3              0.074803         508                23.766667   \n",
       "4              0.023529         255                26.142857   \n",
       "..                  ...         ...                      ...   \n",
       "142            0.038168         655                22.790698   \n",
       "143            0.045759         896                22.530303   \n",
       "144            0.066667         240                24.176471   \n",
       "145            0.027972         143                41.800000   \n",
       "146            0.031088         193                25.583333   \n",
       "\n",
       "     Percentage of Complex Words  Fog Index  Average Words Per Sentence  \\\n",
       "0                      26.388889  23.646465                   32.727273   \n",
       "1                      17.184466  17.715892                   27.105263   \n",
       "2                      22.029703  21.242650                   31.076923   \n",
       "3                      33.239832  22.802599                   23.766667   \n",
       "4                      24.863388  20.402498                   26.142857   \n",
       "..                           ...        ...                         ...   \n",
       "142                    21.938776  17.891789                   22.790698   \n",
       "143                    16.879623  15.763971                   22.530303   \n",
       "144                    18.248175  16.969858                   24.176471   \n",
       "145                    28.708134  28.203254                   41.800000   \n",
       "146                    24.104235  19.875027                   25.583333   \n",
       "\n",
       "     Complex Word Count  Personal Pronouns  Average Word Length  \\\n",
       "0                    95                  2             5.936111   \n",
       "1                   177                  8             5.053398   \n",
       "2                    89                  2             5.784653   \n",
       "3                   237                  5             6.375877   \n",
       "4                    91                  2             5.765027   \n",
       "..                  ...                ...                  ...   \n",
       "142                 215                  4             5.497959   \n",
       "143                 251                 13             5.255548   \n",
       "144                  75                 12             5.284672   \n",
       "145                  60                  1             6.325359   \n",
       "146                  74                  2             5.625407   \n",
       "\n",
       "     Avg Syllables Per Word                           Syllable Counts Per Word  \n",
       "0                  1.941667  1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...  \n",
       "1                  1.657282  3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...  \n",
       "2                  1.866337  1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...  \n",
       "3                  2.107994  3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...  \n",
       "4                  1.923497  4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "..                      ...                                                ...  \n",
       "142                1.845918  4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...  \n",
       "143                1.715535  1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "144                1.749392  2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...  \n",
       "145                2.110048  2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...  \n",
       "146                1.882736  3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...  \n",
       "\n",
       "[147 rows x 15 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a366dba1-c75e-465d-aae8-451d86d69d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"Input.xlsx - Sheet1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "result[\"URL\"]=df[\"URL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f68ad28b-86c7-4a27-b12e-5bf7c8186e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Average Sentence Length</th>\n",
       "      <th>Percentage of Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Average Words Per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Personal Pronouns</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>Avg Syllables Per Word</th>\n",
       "      <th>Syllable Counts Per Word</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.035433</td>\n",
       "      <td>254</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>26.388889</td>\n",
       "      <td>23.646465</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>5.936111</td>\n",
       "      <td>1.941667</td>\n",
       "      <td>1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>15</td>\n",
       "      <td>-8</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.044146</td>\n",
       "      <td>521</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>17.184466</td>\n",
       "      <td>17.715892</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>177</td>\n",
       "      <td>8</td>\n",
       "      <td>5.053398</td>\n",
       "      <td>1.657282</td>\n",
       "      <td>3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>13</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>292</td>\n",
       "      <td>31.076923</td>\n",
       "      <td>22.029703</td>\n",
       "      <td>21.242650</td>\n",
       "      <td>31.076923</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>5.784653</td>\n",
       "      <td>1.866337</td>\n",
       "      <td>1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>26</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.074803</td>\n",
       "      <td>508</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>33.239832</td>\n",
       "      <td>22.802599</td>\n",
       "      <td>23.766667</td>\n",
       "      <td>237</td>\n",
       "      <td>5</td>\n",
       "      <td>6.375877</td>\n",
       "      <td>2.107994</td>\n",
       "      <td>3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>255</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>24.863388</td>\n",
       "      <td>20.402498</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>5.765027</td>\n",
       "      <td>1.923497</td>\n",
       "      <td>4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Netclan20241159</td>\n",
       "      <td>15</td>\n",
       "      <td>-10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>655</td>\n",
       "      <td>22.790698</td>\n",
       "      <td>21.938776</td>\n",
       "      <td>17.891789</td>\n",
       "      <td>22.790698</td>\n",
       "      <td>215</td>\n",
       "      <td>4</td>\n",
       "      <td>5.497959</td>\n",
       "      <td>1.845918</td>\n",
       "      <td>4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Netclan20241160</td>\n",
       "      <td>23</td>\n",
       "      <td>-18</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.045759</td>\n",
       "      <td>896</td>\n",
       "      <td>22.530303</td>\n",
       "      <td>16.879623</td>\n",
       "      <td>15.763971</td>\n",
       "      <td>22.530303</td>\n",
       "      <td>251</td>\n",
       "      <td>13</td>\n",
       "      <td>5.255548</td>\n",
       "      <td>1.715535</td>\n",
       "      <td>1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Netclan20241161</td>\n",
       "      <td>9</td>\n",
       "      <td>-7</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>240</td>\n",
       "      <td>24.176471</td>\n",
       "      <td>18.248175</td>\n",
       "      <td>16.969858</td>\n",
       "      <td>24.176471</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "      <td>5.284672</td>\n",
       "      <td>1.749392</td>\n",
       "      <td>2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Netclan20241162</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>143</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>28.708134</td>\n",
       "      <td>28.203254</td>\n",
       "      <td>41.800000</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>6.325359</td>\n",
       "      <td>2.110048</td>\n",
       "      <td>2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Netclan20241163</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.031088</td>\n",
       "      <td>193</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>24.104235</td>\n",
       "      <td>19.875027</td>\n",
       "      <td>25.583333</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>5.625407</td>\n",
       "      <td>1.882736</td>\n",
       "      <td>3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           File Name  Positive Score  Negative Score  Polarity Score  \\\n",
       "0    Netclan20241017               8              -1        0.777778   \n",
       "1    Netclan20241018              15              -8        0.304348   \n",
       "2    Netclan20241019              13              -3        0.625000   \n",
       "3    Netclan20241020              26             -12        0.368421   \n",
       "4    Netclan20241021               5              -1        0.666667   \n",
       "..               ...             ...             ...             ...   \n",
       "142  Netclan20241159              15             -10        0.200000   \n",
       "143  Netclan20241160              23             -18        0.121951   \n",
       "144  Netclan20241161               9              -7        0.125000   \n",
       "145  Netclan20241162               3              -1        0.500000   \n",
       "146  Netclan20241163               5              -1        0.666667   \n",
       "\n",
       "     Subjectivity Score  Word Count  Average Sentence Length  \\\n",
       "0              0.035433         254                32.727273   \n",
       "1              0.044146         521                27.105263   \n",
       "2              0.054795         292                31.076923   \n",
       "3              0.074803         508                23.766667   \n",
       "4              0.023529         255                26.142857   \n",
       "..                  ...         ...                      ...   \n",
       "142            0.038168         655                22.790698   \n",
       "143            0.045759         896                22.530303   \n",
       "144            0.066667         240                24.176471   \n",
       "145            0.027972         143                41.800000   \n",
       "146            0.031088         193                25.583333   \n",
       "\n",
       "     Percentage of Complex Words  Fog Index  Average Words Per Sentence  \\\n",
       "0                      26.388889  23.646465                   32.727273   \n",
       "1                      17.184466  17.715892                   27.105263   \n",
       "2                      22.029703  21.242650                   31.076923   \n",
       "3                      33.239832  22.802599                   23.766667   \n",
       "4                      24.863388  20.402498                   26.142857   \n",
       "..                           ...        ...                         ...   \n",
       "142                    21.938776  17.891789                   22.790698   \n",
       "143                    16.879623  15.763971                   22.530303   \n",
       "144                    18.248175  16.969858                   24.176471   \n",
       "145                    28.708134  28.203254                   41.800000   \n",
       "146                    24.104235  19.875027                   25.583333   \n",
       "\n",
       "     Complex Word Count  Personal Pronouns  Average Word Length  \\\n",
       "0                    95                  2             5.936111   \n",
       "1                   177                  8             5.053398   \n",
       "2                    89                  2             5.784653   \n",
       "3                   237                  5             6.375877   \n",
       "4                    91                  2             5.765027   \n",
       "..                  ...                ...                  ...   \n",
       "142                 215                  4             5.497959   \n",
       "143                 251                 13             5.255548   \n",
       "144                  75                 12             5.284672   \n",
       "145                  60                  1             6.325359   \n",
       "146                  74                  2             5.625407   \n",
       "\n",
       "     Avg Syllables Per Word  \\\n",
       "0                  1.941667   \n",
       "1                  1.657282   \n",
       "2                  1.866337   \n",
       "3                  2.107994   \n",
       "4                  1.923497   \n",
       "..                      ...   \n",
       "142                1.845918   \n",
       "143                1.715535   \n",
       "144                1.749392   \n",
       "145                2.110048   \n",
       "146                1.882736   \n",
       "\n",
       "                              Syllable Counts Per Word  \\\n",
       "0    1, 1, 1, 1, 2, 4, 1, 2, 2, 1, 1, 4, 3, 4, 1, 2...   \n",
       "1    3, 1, 1, 2, 1, 5, 1, 2, 2, 3, 1, 2, 4, 1, 2, 3...   \n",
       "2    1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 2, 4...   \n",
       "3    3, 3, 1, 4, 1, 3, 2, 1, 1, 1, 3, 3, 4, 1, 3, 2...   \n",
       "4    4, 1, 1, 2, 1, 3, 2, 3, 2, 4, 2, 2, 1, 2, 3, 3...   \n",
       "..                                                 ...   \n",
       "142  4, 1, 4, 2, 1, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3...   \n",
       "143  1, 1, 2, 2, 4, 1, 3, 3, 2, 4, 2, 2, 1, 2, 3, 3...   \n",
       "144  2, 2, 4, 3, 2, 4, 2, 2, 1, 2, 3, 3, 3, 1, 3, 2...   \n",
       "145  2, 1, 1, 2, 2, 2, 1, 3, 2, 4, 2, 2, 1, 2, 3, 3...   \n",
       "146  3, 1, 1, 1, 4, 1, 1, 1, 2, 2, 3, 2, 4, 2, 2, 1...   \n",
       "\n",
       "                                                   URL  \n",
       "0    https://insights.blackcoffer.com/ai-and-ml-bas...  \n",
       "1    https://insights.blackcoffer.com/enhancing-fro...  \n",
       "2    https://insights.blackcoffer.com/roas-dashboar...  \n",
       "3    https://insights.blackcoffer.com/efficient-pro...  \n",
       "4    https://insights.blackcoffer.com/development-o...  \n",
       "..                                                 ...  \n",
       "142  https://insights.blackcoffer.com/population-an...  \n",
       "143  https://insights.blackcoffer.com/google-lsa-ap...  \n",
       "144  https://insights.blackcoffer.com/healthcare-da...  \n",
       "145  https://insights.blackcoffer.com/budget-sales-...  \n",
       "146  https://insights.blackcoffer.com/amazon-buy-bo...  \n",
       "\n",
       "[147 rows x 16 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "399a7fbc-f563-49ab-9062-bf8afc6a02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Output Data STructure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae20c2-2405-4e36-8f98-f04fa13e82f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
